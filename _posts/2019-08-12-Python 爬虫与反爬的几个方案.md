---
layout:     post
title:      Python 爬虫与反爬的几个方案
subtitle:   Python 爬虫与反爬的几个方案
date:       2019-08-12
author:     he xiaodong
header-img: img/default-post-bg.jpg
catalog: true
tags:
    - Python
    - Nginx
    - 爬虫
    - 反爬虫
    - 防火墙
    - 数据扰乱
    - 蜜罐数据
---

> 没有绝对的反爬虫措施，只能提高爬虫爬取的成本。

爬虫措施：<br />
- 不设防的网站，直接爬取，不做任何伪装
- 基础防备的网站，爬取过程中增加 `time.sleep(n)` 进行休眠一下，降级爬取频次，防止被限制。再可以每次爬取切换 header 头信息，伪装成多个终端发起的请求
- 需要登录的情况下，需要多个用户账户，爬取过程中切换 cookie 信息，模拟不同用户在请求。
- 使用 IP 代理池，切换 IP，越过高级限制。

python 爬虫相关的有几个实现越过限制的包：fake_useragent proxy_list 等，伪装的越像真实用户越爬取成功率高。

反爬虫措施：<br />
- Nginx 层面进行频次限制，可以参考 [Nginx http 资源请求限制](https://alpha2016.github.io/2019/05/22/Nginx-http%E8%B5%84%E6%BA%90%E8%AF%B7%E6%B1%82%E9%99%90%E5%88%B6/)，至于限制的 key 为浏览器头/IP/登录用户，可以根据需求进行设置。
- 代码层面限制需要登录访问，一天仅可以访问一定数量的页面，未登录状态仅可以查看可数的几个页面，例如房源信息网站，一天查看 60+ 的房源详情页面已经很多了，更多可以就有恶意了。这样的限制对于爬虫方来说，需要筹备很多账号进行爬取。**当然在彻底的限制之外，可以限制访问超过数量弹出验证码，验证之后才可以继续访问，这样至少不会让少部分真实用户无法访问**。
- 提前获取 IP 代理池的 IP 列表，直接防火墙层面的拉黑，能高端避免一些问题，免费 IP 代理池记得有网站，需要拉黑在自己获取。
- 将常见的爬虫头信息全部 Nginx 或者代码层面拉黑，据说一些大网站把 python 的几个常见爬虫头信息全部拉黑了，提升基础爬虫的代码成本。
- 高端反爬虫是每隔几小时切换页面代码或者接口数据结构，记得淘宝是这样做的，对于爬虫方来说，可能刚刚写好爬这种类型的代码，然后整体页面代码和数据结构用了新一套，很高阶的反制措施了。
- 数据扰乱：每一页有一些加解密规则，或者每页有不同的扰乱数据，你抓取到的极有可能是包含一些假数据，或者加密数据，也算是增加了爬虫成本。例如网页中也可以增加一些关键性的样式或者名称一致的隐藏域，偶数页不出现这些隐藏域，让爬虫不好找的关键元素。

以上是自己想到的一些措施，谨记现在乱爬是违法的，别爬取一时爽，三年起步了。

© 原创文章，如有问题请联系 hexiaodong1992@outlook.com 谢谢!


最后恰饭 [阿里云全系列产品/短信包特惠购买 中小企业上云最佳选择 阿里云内部优惠券](https://www.aliyun.com/minisite/goods?userCode=0amqgcs9)
